{
  "AllowedHosts": "*",
  "Kestrel": {
    "Endpoints": {
      "Http": {
        "Url": "http://*:9001"
      }
    }
  },
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    },
    "Console": {
      "LogToStandardErrorThreshold": "Critical",
      "FormatterName": "simple",
      "FormatterOptions": {
        "TimestampFormat": "[HH:mm:ss.fff] ",
        "SingleLine": true,
        "UseUtcTimestamp": false,
        "IncludeScopes": false,
        "JsonWriterOptions": {
          "Indented": true
        }
      }
    }
  },
  "KernelMemory": {
    "Service": {
      "RunWebService": true,
      "OpenApiEnabled": true,
      "RunHandlers": true,
      "Handlers": {
        "extract": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextExtractionHandler"
        },
        "partition": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextPartitioningHandler"
        },
        "gen_embeddings": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler"
        },
        "save_records": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SaveRecordsHandler"
        },
        "summarize": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SummarizationHandler"
        },
        "delete_generated_files": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteGeneratedFilesHandler"
        },
        "private_delete_document": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteDocumentHandler"
        },
        "private_delete_index": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteIndexHandler"
        },
        "disabled_handler_example": {
          "Class": "",
          "Assembly": ""
        }
      }
    },
    "ServiceAuthorization": {
      "Enabled": false,
      "AuthenticationType": "APIKey",
      "HttpHeaderName": "Authorization",
      "AccessKey1": "",
      "AccessKey2": ""
    },
    // "AzureBlobs", "AWSS3", or "SimpleFileStorage"
    "DocumentStorageType": "SimpleFileStorage",
    "TextGeneratorType": "",
    "DefaultIndexName": "default",
    "DataIngestion": {
      "OrchestrationType": "Distributed",
      "DistributedOrchestration": {
        "QueueType": "SimpleQueues"
      },
      "EmbeddingGenerationEnabled": true,
      "EmbeddingGeneratorTypes": [],
      "MemoryDbTypes": [
        "SimpleVectorDb"
      ],
      "MemoryDbUpsertBatchSize": 1,
      "ImageOcrType": "None",
      "TextPartitioning": {
        "MaxTokensPerLine": 300,
        "MaxTokensPerParagraph": 1000,
        "OverlappingTokens": 100
      },
      "DefaultSteps": []
    },
    "Retrieval": {
      "EmbeddingGeneratorType": "",
      "MemoryDbType": "SimpleVectorDb",
      "SearchClient": {
        "MaxAskPromptSize": -1,
        "MaxMatchesCount": 100,
        "AnswerTokens": 300,
        "EmptyAnswer": "INFO NOT FOUND",
        "Temperature": 0,
        "TopP": 0,
        "PresencePenalty": 0,
        "FrequencyPenalty": 0,
        "StopSequences": []
      }
    },
    "Services": {
      "Anthropic": {
        "Endpoint": "https://api.anthropic.com",
        "EndpointVersion": "2023-06-01",
        "ApiKey": "",
        "TextModelName": "claude-3-haiku-20240307",
        "MaxTokenIn": 200000,
        "MaxTokenOut": 4096,
        "DefaultSystemPrompt": "You are an assistant that will answer user query based on a context",
        "HttpClientName": ""
      },
      "AWSS3": {
        "Auth": "AccessKey",
        // AccessKey ID, required when using AccessKey auth
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__AccessKey' to set this
        "AccessKey": "",
        // SecretAccessKey, required when using AccessKey auth
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__SecretAccessKey' to set this
        "SecretAccessKey": "",
        // Required bucket name where to create directories and upload files.
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__BucketName' to set this
        "BucketName": ""
        // Allows to specify a custom AWS or a compatible endpoint
        // Examples: "https://s3.amazonaws.com", "https://s3.us-west-2.amazonaws.com", "http://127.0.0.1:9444"
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__Endpoint' to set this
        // Note: you can test locally using S3 Ninja https://s3ninja.net
        // "Endpoint": "https://s3.amazonaws.com"
      },
      "AzureAISearch": {
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>",
        "APIKey": "",
        "UseHybridSearch": false
      },
      "AzureAIDocIntel": {
        "Auth": "AzureIdentity",
        "APIKey": "",
        "Endpoint": ""
      },
      "AzureBlobs": {
        "Auth": "AzureIdentity",
        "Account": "",
        "Container": "smemory",
        "ConnectionString": "",
        "EndpointSuffix": "core.windows.net"
      },
      "AzureOpenAIEmbedding": {
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 8191,
        // The number of dimensions output embeddings should have.
        // Only supported in "text-embedding-3" and later models developed with
        // MRL, see https://arxiv.org/abs/2205.13147
        "EmbeddingDimensions": null,
        // How many embeddings to calculate in parallel. The max value depends on
        // the model and deployment in use.
        // See https://learn.microsoft.com/azure/ai-services/openai/reference#embeddings
        "MaxEmbeddingBatchSize": 1,
        // How many times to retry in case of throttling.
        "MaxRetries": 10
      },
      "AzureOpenAIText": {
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        "APIType": "ChatCompletion",
        // How many times to retry in case of throttling.
        "MaxRetries": 10
      },
      "AzureQueues": {
        "Auth": "AzureIdentity",
        "Account": "",
        "ConnectionString": "",
        "EndpointSuffix": "core.windows.net",
        "PollDelayMsecs": 100,
        "FetchBatchSize": 3,
        "FetchLockSeconds": 300,
        "MaxRetriesBeforePoisonQueue": 20,
        "PoisonQueueSuffix": "-poison"
      },
      "Elasticsearch": {
        "CertificateFingerPrint": "",
        "Endpoint": "",
        "UserName": "",
        "Password": "",
        "IndexPrefix": "",
        "ShardCount": 1,
        "Replicas": 0
      },
      "LlamaSharp": {
        "ModelPath": "",
        "MaxTokenTotal": 4096
      },
      "MongoDbAtlas": {
        "ConnectionString": "mongodb://root:root@localhost:27777/?authSource=admin",
        "DatabaseName": "KernelMemory",
        "UseSingleCollectionForVectorSearch": false
      },
      "Ollama": {
        "Endpoint": "http://localhost:11434",
        "TextModel": {
          "ModelName": "phi3:medium-128k",
          "MaxTokenTotal": 131072,
          // How many requests can be processed in parallel
          "MaxBatchSize": 1
          //// Enable Mirostat sampling for controlling perplexity.
          //// (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
          //"MiroStat": 0,
          //// Influences how quickly the algorithm responds to feedback from the
          //// generated text. A lower learning rate will result in slower adjustments,
          //// while a higher learning rate will make the algorithm more responsive.
          //// (Default: 0.1)
          //"MiroStatEta": 0.1,
          //// Controls the balance between coherence and diversity of the output.
          //// A lower value will result in more focused and coherent text.
          //// (Default: 5.0)
          //"MiroStatTau": 5.0,
          //// Sets the size of the context window used to generate the next token.
          //// (Default: 2048)
          //"NumCtx": 2048,
          //// The number of GQA groups in the transformer layer. Required for some
          //// models, for example it is 8 for llama2:70b
          //"NumGqa": null,
          //// The number of layers to send to the GPU(s). On macOS it defaults to
          //// 1 to enable metal support, 0 to disable.
          //"NumGpu": null,
          //// Sets the number of threads to use during computation. By default,
          //// Ollama will detect this for optimal performance.
          //// It is recommended to set this value to the number of physical CPU cores
          //// your system has (as opposed to the logical number of cores).
          //"NumThread": null,
          //// Sets how far back for the model to look back to prevent repetition.
          //// (Default: 64, 0 = disabled, -1 = num_ctx)
          //"RepeatLastN": null,
          //// Sets the random number seed to use for generation.
          //// Setting this to a specific number will make the model generate the same
          //// text for the same prompt. (Default: 0)
          //"Seed": 0,
          //// Tail free sampling is used to reduce the impact of less probable
          //// tokens from the output. A higher value (e.g., 2.0) will reduce the
          //// impact more, while a value of 1.0 disables this setting. (default: 1)
          //"TfsZ": 1.0,
          //// Maximum number of tokens to predict when generating text.
          //// (Default: 128, -1 = infinite generation, -2 = fill context)
          //"NumPredict": 128,
          //// Reduces the probability of generating nonsense. A higher value
          //// (e.g. 100) will give more diverse answers, while a lower value (e.g. 10)
          //// will be more conservative. (Default: 40)
          //"TopK": 40,
          //// Alternative to the top_p, and aims to ensure a balance of quality and variety.min_p represents the minimum
          //// probability for a token to be considered, relative to the probability of the most likely token.For
          //// example, with min_p=0.05 and the most likely token having a probability of 0.9, logits with a value less
          //// than 0.05*0.9=0.045 are filtered out. (Default: 0.0)
          //"MinP": 0.0
        },
        "EmbeddingModel": {
          "ModelName": "nomic-embed-text",
          "MaxTokenTotal": 2048,
          // How many requests can be processed in parallel
          "MaxBatchSize": 1
          //// Enable Mirostat sampling for controlling perplexity.
          //// (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
          //"MiroStat": 0,
          //// Influences how quickly the algorithm responds to feedback from the
          //// generated text. A lower learning rate will result in slower adjustments,
          //// while a higher learning rate will make the algorithm more responsive.
          //// (Default: 0.1)
          //"MiroStatEta": 0.1,
          //// Controls the balance between coherence and diversity of the output.
          //// A lower value will result in more focused and coherent text.
          //// (Default: 5.0)
          //"MiroStatTau": 5.0,
          //// Sets the size of the context window used to generate the next token.
          //// (Default: 2048)
          //"NumCtx": 2048,
          //// The number of GQA groups in the transformer layer. Required for some
          //// models, for example it is 8 for llama2:70b
          //"NumGqa": null,
          //// The number of layers to send to the GPU(s). On macOS it defaults to
          //// 1 to enable metal support, 0 to disable.
          //"NumGpu": null,
          //// Sets the number of threads to use during computation. By default,
          //// Ollama will detect this for optimal performance.
          //// It is recommended to set this value to the number of physical CPU cores
          //// your system has (as opposed to the logical number of cores).
          //"NumThread": null,
          //// Sets how far back for the model to look back to prevent repetition.
          //// (Default: 64, 0 = disabled, -1 = num_ctx)
          //"RepeatLastN": null,
          //// Sets the random number seed to use for generation.
          //// Setting this to a specific number will make the model generate the same
          //// text for the same prompt. (Default: 0)
          //"Seed": 0,
          //// Tail free sampling is used to reduce the impact of less probable
          //// tokens from the output. A higher value (e.g., 2.0) will reduce the
          //// impact more, while a value of 1.0 disables this setting. (default: 1)
          //"TfsZ": 1.0,
          //// Maximum number of tokens to predict when generating text.
          //// (Default: 128, -1 = infinite generation, -2 = fill context)
          //"NumPredict": 128,
          //// Reduces the probability of generating nonsense. A higher value
          //// (e.g. 100) will give more diverse answers, while a lower value (e.g. 10)
          //// will be more conservative. (Default: 40)
          //"TopK": 40,
          //// Alternative to the top_p, and aims to ensure a balance of quality and variety.min_p represents the minimum
          //// probability for a token to be considered, relative to the probability of the most likely token.For
          //// example, with min_p=0.05 and the most likely token having a probability of 0.9, logits with a value less
          //// than 0.05*0.9=0.045 are filtered out. (Default: 0.0)
          //"MinP": 0.0
        }
      },
      "OpenAI": {
        // Name of the model used to generate text (text completion or chat completion)
        "TextModel": "gpt-4o-mini",
        // The max number of tokens supported by the text model.
        "TextModelMaxTokenTotal": 16384,
        "TextGenerationType": "Auto",
        "EmbeddingModel": "text-embedding-ada-002",
        "EmbeddingModelMaxTokenTotal": 8191,
        "APIKey": "",
        "OrgId": "",
        "Endpoint": "",
        "MaxRetries": 10,
        // The number of dimensions output embeddings should have.
        // Only supported in "text-embedding-3" and later models developed with
        // MRL, see https://arxiv.org/abs/2205.13147
        "EmbeddingDimensions": null,
        // How many embeddings to calculate in parallel.
        // See https://platform.openai.com/docs/api-reference/embeddings/create
        "MaxEmbeddingBatchSize": 100
      },
      "Postgres": {
        // Postgres instance connection string
        "ConnectionString": "Host=localhost;Port=5432;Username=public;Password=;Database=public",
        // Mandatory prefix to add to the name of table managed by KM,
        // e.g. to exclude other tables in the same schema.
        "EmbeddingDimensions": null,
        "TableNamePrefix": "km-"
      },
      "Qdrant": {
        "Endpoint": "http://127.0.0.1:6333",
        "APIKey": ""
      },
      "RabbitMQ": {
        "Host": "127.0.0.1",
        "Port": "5672",
        "Username": "user",
        "Password": "",
        "VirtualHost": "/",
        "MessageTTLSecs": 3600,
        "SslEnabled": false
      },
      "Redis": {
        "ConnectionString": "",
        "Tags": {
          "type": ",",
          "user": ",",
          "ext": ","
        }
      },
      "SimpleFileStorage": {
        "StorageType": "Volatile",
        "Directory": "_files"
      },
      "SimpleQueues": {
        "StorageType": "Volatile",
        "Directory": "_queues"
      },
      "SimpleVectorDb": {
        "StorageType": "Volatile",
        "Directory": "_vectors"
      },
      "SqlServer": {
        "ConnectionString": "",
        "Schema": "dbo",
        "MemoryCollectionTableName": "KMCollections",
        "MemoryTableName": "KMMemories",
        "EmbeddingsTableName": "KMEmbeddings",
        "TagsTableName": "KMMemoriesTags",
        // See https://devblogs.microsoft.com/azure-sql/announcing-eap-native-vector-support-in-azure-sql-database
        "UseNativeVectorSearch": false
      }
    }
  },
  "ApplicationInsights": {
    "ConnectionString": "InstrumentationKey=e2fe0cdb-6382-49b1-8e04-8ecd4375d5dc;IngestionEndpoint=https://centralindia-0.in.applicationinsights.azure.com/;LiveEndpoint=https://centralindia.livediagnostics.monitor.azure.com/;ApplicationId=19d89e23-5596-472f-9772-60e654065fa8"
  }
}
